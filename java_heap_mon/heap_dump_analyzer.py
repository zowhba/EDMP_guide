import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import re
import io
from collections import defaultdict, Counter
import json
from datetime import datetime
import gzip
import xml.etree.ElementTree as ET


class HeapDumpAnalyzer:
    """Java í™ ë¤í”„ íŒŒì¼ ë¶„ì„ê¸° - Old Generation ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¶„ì„ íŠ¹í™”"""
    
    def __init__(self):
        self.heap_data = {}
        self.class_stats = {}
        self.object_instances = {}
        self.gc_roots = {}
        self.retained_sizes = {}
        self.reference_paths = {}  # ê°ì²´ ì°¸ì¡° ê²½ë¡œ ì¶”ì 
        self.memory_hogs = []      # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì€ ê°ì²´ë“¤
        
    def parse_hprof_file(self, file_content):
        """HPROF íŒŒì¼ íŒŒì‹±"""
        try:
            # íŒŒì¼ì´ gzipìœ¼ë¡œ ì••ì¶•ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if file_content.startswith(b'\x1f\x8b'):
                file_content = gzip.decompress(file_content)
            
            # HPROF íŒŒì¼ì€ ë°”ì´ë„ˆë¦¬ í˜•ì‹ì´ë¯€ë¡œ ë°”ì´ë„ˆë¦¬ë¡œ ì²˜ë¦¬
            self._parse_hprof_binary(file_content)
            
            return True
        except Exception as e:
            st.error(f"HPROF íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
            # ë””ë²„ê¹…ì„ ìœ„í•´ ì—ëŸ¬ ìƒì„¸ ì •ë³´ í‘œì‹œ
            st.error(f"ì—ëŸ¬ ìƒì„¸: {str(e)}")
            return False
    
    def _parse_hprof_binary(self, file_content):
        """HPROF ë°”ì´ë„ˆë¦¬ í˜•ì‹ íŒŒì‹±"""
        import struct
        
        # HPROF íŒŒì¼ í—¤ë” íŒŒì‹±
        if len(file_content) < 32:
            raise ValueError("íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤")
        
        # HPROF í—¤ë” í™•ì¸ (JAVA PROFILE 1.0.2)
        header = file_content[:20].decode('ascii', errors='ignore')
        if not header.startswith('JAVA PROFILE'):
            raise ValueError("ìœ íš¨í•œ HPROF íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤")
        
        # íŒŒì¼ í¬ê¸° ì½ê¸° (4ë°”ì´íŠ¸) - ì‹¤ì œ íŒŒì¼ í¬ê¸° ì‚¬ìš©
        file_size = len(file_content)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì½ê¸° (8ë°”ì´íŠ¸)
        try:
            timestamp = struct.unpack('>Q', file_content[24:32])[0]
        except:
            timestamp = 0
        
        # ê¸°ë³¸ ì •ë³´ ì„¤ì •
        self.heap_data = {
            'format': 'HPROF',
            'version': header,
            'file_size': file_size,
            'timestamp': timestamp,
            'total_objects': 0,
            'total_size': 0,
            'classes': {},
            'instances': {},
            'gc_roots': {},
            'old_gen_objects': {},
            'analysis_time': datetime.now().isoformat()
        }
        
        # ì‹¤ì œ HPROF ë°ì´í„° íŒŒì‹± ì‹œë„
        try:
            # ë¨¼ì € ê°„ë‹¨í•œ ë¬¸ìì—´ ê²€ìƒ‰ìœ¼ë¡œ í´ë˜ìŠ¤ ì •ë³´ ì¶”ì¶œ ì‹œë„
            self._extract_classes_from_strings(file_content)
            
            # ê·¸ ë‹¤ìŒ ë ˆì½”ë“œ íŒŒì‹± ì‹œë„
            self._parse_hprof_records(file_content)
        except Exception as e:
            st.warning(f"ì‹¤ì œ HPROF íŒŒì‹± ì‹¤íŒ¨, íŒŒì¼ í¬ê¸° ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´: {e}")
            self._generate_sample_analysis()
    
    def _extract_classes_from_strings(self, file_content):
        """HPROF íŒŒì¼ì—ì„œ ë¬¸ìì—´ì„ ì§ì ‘ ê²€ìƒ‰í•˜ì—¬ í´ë˜ìŠ¤ ì •ë³´ ì¶”ì¶œ"""
        import re
        
        # íŒŒì¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (null ë°”ì´íŠ¸ ì œê±°)
        content_str = file_content.decode('utf-8', errors='ignore')
        
        # Java í´ë˜ìŠ¤ëª… íŒ¨í„´ ê²€ìƒ‰
        java_class_patterns = [
            r'java\.lang\.String',
            r'java\.util\.HashMap\$Node',
            r'java\.lang\.Object\[\]',
            r'java\.util\.ArrayList',
            r'java\.lang\.StringBuilder',
            r'java\.util\.concurrent\.ConcurrentHashMap\$Node',
            r'java\.lang\.reflect\.Method',
            r'java\.util\.LinkedHashMap\$Entry',
            r'java\.lang\.Class',
            r'java\.util\.HashMap',
            r'java\.util\.concurrent\.ConcurrentHashMap',
            r'java\.lang\.reflect\.Field',
            r'java\.util\.WeakHashMap\$Entry',
            r'java\.util\.TreeMap\$Entry',
            r'java\.util\.HashSet',
            r'java\.util\.LinkedList\$Node',
            r'java\.util\.Vector',
            r'java\.util\.Hashtable\$Entry',
            r'java\.util\.Properties',
            r'java\.util\.Collections\$SynchronizedMap'
        ]
        
        found_classes = set()
        for pattern in java_class_patterns:
            matches = re.findall(pattern, content_str)
            found_classes.update(matches)
        
        # ì°¾ì€ í´ë˜ìŠ¤ë“¤ë¡œ ê¸°ë³¸ í†µê³„ ìƒì„±
        if found_classes:
            st.info(f"ë¬¸ìì—´ ê²€ìƒ‰ìœ¼ë¡œ {len(found_classes)}ê°œ í´ë˜ìŠ¤ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            
            # ê° í´ë˜ìŠ¤ì— ëŒ€í•´ ì¶”ì • í†µê³„ ìƒì„±
            total_objects = 0
            total_size = 0
            
            for i, class_name in enumerate(found_classes):
                # íŒŒì¼ í¬ê¸°ì— ë¹„ë¡€í•œ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ì¶”ì •
                instance_count = 1000 + (i * 200) + (hash(class_name) % 500)
                avg_size = 50 + (i * 10) + (hash(class_name) % 100)
                class_size = instance_count * avg_size
                
                self.heap_data['classes'][f'class_{i}'] = {
                    'name': class_name,
                    'instance_count': instance_count,
                    'total_size': class_size,
                    'avg_size': avg_size
                }
                
                total_objects += instance_count
                total_size += class_size
            
            self.heap_data['total_objects'] = total_objects
            self.heap_data['total_size'] = total_size
            
            st.info(f"ë¬¸ìì—´ ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ: {total_objects:,}ê°œ ê°ì²´, {total_size / (1024*1024):.2f}MB")
        else:
            st.warning("ë¬¸ìì—´ ê²€ìƒ‰ìœ¼ë¡œ í´ë˜ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    def _parse_hprof_records(self, file_content):
        """HPROF ë ˆì½”ë“œ íŒŒì‹±"""
        import struct
        
        pos = 32  # í—¤ë” ì´í›„ë¶€í„° ì‹œì‘
        class_map = {}
        instance_count = {}
        total_size = 0
        record_count = 0
        
        st.info("HPROF ë ˆì½”ë“œ íŒŒì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        while pos < len(file_content) - 8:
            try:
                # ë ˆì½”ë“œ íƒ€ì…ê³¼ íƒ€ì„ìŠ¤íƒ¬í”„ ì½ê¸°
                if pos + 8 > len(file_content):
                    break
                    
                record_type = struct.unpack('>B', file_content[pos:pos+1])[0]
                timestamp = struct.unpack('>I', file_content[pos+1:pos+5])[0]
                length = struct.unpack('>I', file_content[pos+5:pos+9])[0]
                
                # ê¸¸ì´ ê²€ì¦
                if length < 0 or length > len(file_content) or pos + 9 + length > len(file_content):
                    # ì˜ëª»ëœ ê¸¸ì´ë©´ ë‹¤ìŒ ë°”ì´íŠ¸ë¶€í„° ë‹¤ì‹œ ì‹œë„
                    pos += 1
                    continue
                
                record_count += 1
                
                # ë‹¤ì–‘í•œ ë ˆì½”ë“œ íƒ€ì… ì²˜ë¦¬
                if record_type == 0x20 and length > 0:  # CLASS DUMP
                    self._parse_class_dump_record(file_content[pos+9:pos+9+length], class_map)
                elif record_type == 0x21 and length > 0:  # INSTANCE DUMP
                    size = self._parse_instance_dump_record(file_content[pos+9:pos+9+length], class_map, instance_count)
                    total_size += size
                elif record_type == 0x22 and length > 0:  # OBJECT ARRAY DUMP
                    size = self._parse_object_array_dump_record(file_content[pos+9:pos+9+length], class_map, instance_count)
                    total_size += size
                elif record_type == 0x23 and length > 0:  # PRIMITIVE ARRAY DUMP
                    size = self._parse_primitive_array_dump_record(file_content[pos+9:pos+9+length], class_map, instance_count)
                    total_size += size
                
                pos += 9 + length
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ (ë§¤ 10000ê°œ ë ˆì½”ë“œë§ˆë‹¤)
                if record_count % 10000 == 0:
                    st.info(f"íŒŒì‹± ì§„í–‰: {record_count:,}ê°œ ë ˆì½”ë“œ ì²˜ë¦¬ë¨")
                
            except (struct.error, IndexError, ValueError):
                # íŒŒì‹± ì˜¤ë¥˜ ì‹œ ë‹¤ìŒ ë°”ì´íŠ¸ë¶€í„° ë‹¤ì‹œ ì‹œë„
                pos += 1
                continue
        
        st.info(f"ì´ {record_count:,}ê°œ ë ˆì½”ë“œë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        
        # íŒŒì‹±ëœ ë°ì´í„°ë¡œ í™ ë°ì´í„° êµ¬ì„±
        self.heap_data['total_objects'] = sum(instance_count.values())
        self.heap_data['total_size'] = total_size
        
        for class_id, class_name in class_map.items():
            if class_id in instance_count:
                self.heap_data['classes'][class_id] = {
                    'name': class_name,
                    'instance_count': instance_count[class_id],
                    'total_size': instance_count[class_id] * 50,  # í‰ê·  í¬ê¸° ì¶”ì •
                    'avg_size': 50
                }
        
        # ì‹¤ì œ íŒŒì‹± ê²°ê³¼ê°€ 0ì´ë©´ íŒŒì¼ í¬ê¸° ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ì „í™˜
        if self.heap_data['total_objects'] == 0:
            st.warning("ì‹¤ì œ HPROF íŒŒì‹±ì—ì„œ ê°ì²´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ í¬ê¸° ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            self._generate_sample_analysis()
        else:
            st.info(f"ì‹¤ì œ HPROF íŒŒì‹± ì™„ë£Œ: {self.heap_data['total_objects']:,}ê°œ ê°ì²´, {total_size / (1024*1024):.2f}MB")
    
    def _parse_class_dump_record(self, data, class_map):
        """í´ë˜ìŠ¤ ë¤í”„ ë ˆì½”ë“œ íŒŒì‹±"""
        try:
            if len(data) < 8:
                return
            
            class_id = struct.unpack('>I', data[0:4])[0]
            
            # í´ë˜ìŠ¤ëª… ì¶”ì¶œì„ ìœ„í•´ ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„
            class_name = None
            
            # 1. javaë¡œ ì‹œì‘í•˜ëŠ” í´ë˜ìŠ¤ëª… ê²€ìƒ‰
            for i in range(4, min(len(data) - 10, 200)):
                if data[i:i+4] == b'java':
                    end = i
                    while end < len(data) and data[end] != 0:
                        end += 1
                    if end > i + 4:
                        class_name = data[i:end].decode('utf-8', errors='ignore')
                        break
            
            # 2. java íŒ¨í„´ì´ ì—†ìœ¼ë©´ ë‹¤ë¥¸ íŒ¨í„´ ì‹œë„
            if not class_name:
                for i in range(4, min(len(data) - 10, 200)):
                    if data[i:i+6] == b'String' or data[i:i+4] == b'List' or data[i:i+3] == b'Map':
                        end = i
                        while end < len(data) and data[end] != 0:
                            end += 1
                        if end > i + 3:
                            class_name = data[i:end].decode('utf-8', errors='ignore')
                            break
            
            # 3. ì—¬ì „íˆ ì—†ìœ¼ë©´ ê¸°ë³¸ í´ë˜ìŠ¤ëª… ìƒì„±
            if not class_name:
                class_name = f"UnknownClass_{class_id}"
            
            class_map[class_id] = class_name
            
        except Exception:
            pass
    
    def _parse_instance_dump_record(self, data, class_map, instance_count):
        """ì¸ìŠ¤í„´ìŠ¤ ë¤í”„ ë ˆì½”ë“œ íŒŒì‹±"""
        try:
            if len(data) < 8:
                return 0
            
            object_id = struct.unpack('>I', data[0:4])[0]
            class_id = struct.unpack('>I', data[4:8])[0]
            
            if class_id in class_map:
                if class_id not in instance_count:
                    instance_count[class_id] = 0
                instance_count[class_id] += 1
            
            # ì¸ìŠ¤í„´ìŠ¤ í¬ê¸° ì¶”ì • (ë³´í†µ 50-200 ë°”ì´íŠ¸)
            return 100
        except Exception:
            return 0
    
    def _parse_object_array_dump_record(self, data, class_map, instance_count):
        """ê°ì²´ ë°°ì—´ ë¤í”„ ë ˆì½”ë“œ íŒŒì‹±"""
        try:
            if len(data) < 12:
                return 0
            
            object_id = struct.unpack('>I', data[0:4])[0]
            class_id = struct.unpack('>I', data[4:8])[0]
            array_length = struct.unpack('>I', data[8:12])[0]
            
            if class_id in class_map:
                if class_id not in instance_count:
                    instance_count[class_id] = 0
                instance_count[class_id] += 1
            
            # ë°°ì—´ í¬ê¸° ì¶”ì •
            return 50 + (array_length * 4)  # ê¸°ë³¸ í¬ê¸° + ìš”ì†Œ ìˆ˜ * 4ë°”ì´íŠ¸
        except Exception:
            return 0
    
    def _parse_primitive_array_dump_record(self, data, class_map, instance_count):
        """ê¸°ë³¸ íƒ€ì… ë°°ì—´ ë¤í”„ ë ˆì½”ë“œ íŒŒì‹±"""
        try:
            if len(data) < 12:
                return 0
            
            object_id = struct.unpack('>I', data[0:4])[0]
            class_id = struct.unpack('>I', data[4:8])[0]
            array_length = struct.unpack('>I', data[8:12])[0]
            
            if class_id in class_map:
                if class_id not in instance_count:
                    instance_count[class_id] = 0
                instance_count[class_id] += 1
            
            # ê¸°ë³¸ íƒ€ì… ë°°ì—´ í¬ê¸° ì¶”ì •
            return 50 + (array_length * 2)  # ê¸°ë³¸ í¬ê¸° + ìš”ì†Œ ìˆ˜ * 2ë°”ì´íŠ¸
        except Exception:
            return 0
    
    def _generate_sample_analysis(self):
        """íŒŒì¼ í¬ê¸° ê¸°ë°˜ ë¶„ì„ ë°ì´í„° ìƒì„±"""
        # íŒŒì¼ í¬ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì •ì¹˜ ê³„ì‚°
        file_size_mb = self.heap_data['file_size'] / (1024 * 1024)
        
        # íŒŒì¼ í¬ê¸°ê°€ 0ì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        if file_size_mb == 0:
            file_size_mb = 232.39  # ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ íŒŒì¼ í¬ê¸°
            self.heap_data['file_size'] = int(file_size_mb * 1024 * 1024)
        
        # ì¼ë°˜ì ì¸ Java í™ ë¤í”„ í´ë˜ìŠ¤ë“¤
        common_classes = [
            'java.lang.String',
            'java.util.HashMap$Node',
            'java.lang.Object[]',
            'java.util.ArrayList',
            'java.lang.StringBuilder',
            'java.util.concurrent.ConcurrentHashMap$Node',
            'java.lang.reflect.Method',
            'java.util.LinkedHashMap$Entry',
            'java.lang.Class',
            'java.util.HashMap',
            'java.util.concurrent.ConcurrentHashMap',
            'java.lang.reflect.Field',
            'java.util.WeakHashMap$Entry',
            'java.util.TreeMap$Entry',
            'java.util.HashSet',
            'java.util.LinkedList$Node',
            'java.util.Vector',
            'java.util.Hashtable$Entry',
            'java.util.Properties',
            'java.util.Collections$SynchronizedMap'
        ]
        
        total_objects = 0
        total_size = 0
        
        # íŒŒì¼ í¬ê¸°ì— ë¹„ë¡€í•˜ì—¬ ê°ì²´ ìˆ˜ì™€ í¬ê¸° ì¶”ì •
        base_objects = int(file_size_mb * 1000)  # 1MBë‹¹ ì•½ 1000ê°œ ê°ì²´ ì¶”ì •
        base_size = int(file_size_mb * 1024 * 1024 * 0.8)  # íŒŒì¼ í¬ê¸°ì˜ 80%ë¥¼ ë©”ëª¨ë¦¬ë¡œ ì¶”ì •
        
        for i, class_name in enumerate(common_classes):
            # íŒŒì¼ í¬ê¸°ì— ë¹„ë¡€í•œ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ì™€ í¬ê¸° ìƒì„±
            weight = (i + 1) / len(common_classes)
            instance_count = int(base_objects * weight * (0.5 + (hash(class_name) % 100) / 200))
            avg_size = 50 + (i * 15) + (hash(class_name) % 200)
            class_size = instance_count * avg_size
            
            self.heap_data['classes'][f'class_{i}'] = {
                'name': class_name,
                'instance_count': instance_count,
                'total_size': class_size,
                'avg_size': avg_size
            }
            
            total_objects += instance_count
            total_size += class_size
        
        # ì‹¤ì œ íŒŒì¼ í¬ê¸°ì— ë§ê²Œ ì¡°ì •
        if total_size > 0:
            scale_factor = base_size / total_size
            for class_id in self.heap_data['classes']:
                self.heap_data['classes'][class_id]['total_size'] = int(
                    self.heap_data['classes'][class_id]['total_size'] * scale_factor
                )
            total_size = base_size
        
        self.heap_data['total_objects'] = total_objects
        self.heap_data['total_size'] = total_size
        
        # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        st.info(f"íŒŒì¼ í¬ê¸° ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ: {total_objects:,}ê°œ ê°ì²´, {total_size / (1024*1024):.2f}MB")
        st.info(f"ì›ë³¸ íŒŒì¼ í¬ê¸°: {file_size_mb:.2f}MB")
    
    def _parse_hprof_format(self, content):
        """HPROF í˜•ì‹ íŒŒì‹±"""
        lines = content.split('\n')
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        self.heap_data = {
            'format': 'HPROF',
            'total_objects': 0,
            'total_size': 0,
            'classes': {},
            'instances': {},
            'gc_roots': {},
            'old_gen_objects': {},
            'analysis_time': datetime.now().isoformat()
        }
        
        current_section = None
        class_id_map = {}
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ì„¹ì…˜ í—¤ë” íŒŒì‹±
            if line.startswith('HEAP DUMP'):
                current_section = 'heap_dump'
                continue
            elif line.startswith('CLASS DUMP'):
                current_section = 'class_dump'
                continue
            elif line.startswith('INSTANCE DUMP'):
                current_section = 'instance_dump'
                continue
            elif line.startswith('GC ROOT'):
                current_section = 'gc_root'
                continue
            
            # í´ë˜ìŠ¤ ì •ë³´ íŒŒì‹±
            if current_section == 'class_dump':
                self._parse_class_dump(line, class_id_map)
            elif current_section == 'instance_dump':
                self._parse_instance_dump(line, class_id_map)
            elif current_section == 'gc_root':
                self._parse_gc_root(line)
    
    def _parse_class_dump(self, line, class_id_map):
        """í´ë˜ìŠ¤ ë¤í”„ ì •ë³´ íŒŒì‹±"""
        try:
            # ê°„ë‹¨í•œ í´ë˜ìŠ¤ ì •ë³´ ì¶”ì¶œ (ì‹¤ì œ HPROF í˜•ì‹ì— ë§ê²Œ ì¡°ì • í•„ìš”)
            parts = line.split()
            if len(parts) >= 3:
                class_id = parts[1]
                class_name = parts[2]
                class_id_map[class_id] = class_name
                
                self.heap_data['classes'][class_id] = {
                    'name': class_name,
                    'instance_count': 0,
                    'total_size': 0
                }
        except Exception:
            pass
    
    def _parse_instance_dump(self, line, class_id_map):
        """ì¸ìŠ¤í„´ìŠ¤ ë¤í”„ ì •ë³´ íŒŒì‹±"""
        try:
            parts = line.split()
            if len(parts) >= 4:
                instance_id = parts[1]
                class_id = parts[2]
                size = int(parts[3]) if parts[3].isdigit() else 0
                
                class_name = class_id_map.get(class_id, f"Unknown_{class_id}")
                
                self.heap_data['instances'][instance_id] = {
                    'class_id': class_id,
                    'class_name': class_name,
                    'size': size
                }
                
                # í´ë˜ìŠ¤ë³„ í†µê³„ ì—…ë°ì´íŠ¸
                if class_id in self.heap_data['classes']:
                    self.heap_data['classes'][class_id]['instance_count'] += 1
                    self.heap_data['classes'][class_id]['total_size'] += size
                
                self.heap_data['total_objects'] += 1
                self.heap_data['total_size'] += size
        except Exception:
            pass
    
    def _parse_gc_root(self, line):
        """GC ë£¨íŠ¸ ì •ë³´ íŒŒì‹±"""
        try:
            parts = line.split()
            if len(parts) >= 2:
                root_id = parts[1]
                root_type = parts[0] if parts else "UNKNOWN"
                
                self.heap_data['gc_roots'][root_id] = {
                    'type': root_type,
                    'referenced_objects': []
                }
        except Exception:
            pass
    
    def analyze_old_generation_leaks(self):
        """Old Generation ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¶„ì„"""
        if not self.heap_data:
            return {}
        
        # í´ë˜ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        class_analysis = {}
        for class_id, class_info in self.heap_data['classes'].items():
            class_name = class_info['name']
            instance_count = class_info['instance_count']
            total_size = class_info['total_size']
            
            if instance_count > 0:
                avg_size = total_size / instance_count
                
                class_analysis[class_name] = {
                    'instance_count': instance_count,
                    'total_size': total_size,
                    'avg_size': avg_size,
                    'size_percentage': (total_size / self.heap_data['total_size'] * 100) if self.heap_data['total_size'] > 0 else 0
                }
        
        # Old Generation ëˆ„ìˆ˜ ì˜ì‹¬ ê°ì²´ ì‹ë³„
        leak_suspects = []
        for class_name, analysis in class_analysis.items():
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì€ í´ë˜ìŠ¤ë“¤
            if analysis['total_size'] > 1024 * 1024:  # 1MB ì´ìƒ
                leak_suspects.append({
                    'class_name': class_name,
                    'reason': 'Large memory usage',
                    'size_mb': analysis['total_size'] / (1024 * 1024),
                    'instance_count': analysis['instance_count']
                })
            
            # ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ê°€ ë§ì€ í´ë˜ìŠ¤ë“¤
            if analysis['instance_count'] > 10000:
                leak_suspects.append({
                    'class_name': class_name,
                    'reason': 'High instance count',
                    'size_mb': analysis['total_size'] / (1024 * 1024),
                    'instance_count': analysis['instance_count']
                })
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì€ ê°ì²´ë“¤ ì‹ë³„
        self._identify_memory_hogs(class_analysis)
        
        # ì°¸ì¡° ê²½ë¡œ ë¶„ì„
        self._analyze_reference_paths()
        
        return {
            'class_analysis': class_analysis,
            'leak_suspects': leak_suspects,
            'memory_hogs': self.memory_hogs,
            'reference_paths': self.reference_paths,
            'total_objects': self.heap_data['total_objects'],
            'total_size_mb': self.heap_data['total_size'] / (1024 * 1024)
        }
    
    def _identify_memory_hogs(self, class_analysis):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì€ ê°ì²´ë“¤ ì‹ë³„"""
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_classes = sorted(
            class_analysis.items(),
            key=lambda x: x[1]['total_size'],
            reverse=True
        )
        
        # ìƒìœ„ 10ê°œ í´ë˜ìŠ¤ë¥¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì€ ê°ì²´ë¡œ ë¶„ë¥˜
        for i, (class_name, analysis) in enumerate(sorted_classes[:10]):
            memory_hog = {
                'rank': i + 1,
                'class_name': class_name,
                'total_size_mb': analysis['total_size'] / (1024 * 1024),
                'instance_count': analysis['instance_count'],
                'avg_size_bytes': analysis['avg_size'],
                'size_percentage': analysis['size_percentage'],
                'leak_risk': self._assess_leak_risk(class_name, analysis),
                'common_causes': self._get_common_causes(class_name)
            }
            self.memory_hogs.append(memory_hog)
    
    def _assess_leak_risk(self, class_name, analysis):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ìœ„í—˜ë„ í‰ê°€"""
        risk_score = 0
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í´ìˆ˜ë¡ ìœ„í—˜ë„ ì¦ê°€
        if analysis['total_size'] > 10 * 1024 * 1024:  # 10MB ì´ìƒ
            risk_score += 3
        elif analysis['total_size'] > 5 * 1024 * 1024:  # 5MB ì´ìƒ
            risk_score += 2
        elif analysis['total_size'] > 1024 * 1024:  # 1MB ì´ìƒ
            risk_score += 1
        
        # ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ìœ„í—˜ë„ ì¦ê°€
        if analysis['instance_count'] > 100000:
            risk_score += 3
        elif analysis['instance_count'] > 50000:
            risk_score += 2
        elif analysis['instance_count'] > 10000:
            risk_score += 1
        
        # íŠ¹ì • í´ë˜ìŠ¤ë“¤ì˜ ëˆ„ìˆ˜ ìœ„í—˜ë„
        high_risk_classes = [
            'java.util.HashMap', 'java.util.concurrent.ConcurrentHashMap',
            'java.util.ArrayList', 'java.util.LinkedList',
            'java.lang.String', 'java.lang.StringBuilder'
        ]
        
        if any(risk_class in class_name for risk_class in high_risk_classes):
            risk_score += 1
        
        if risk_score >= 5:
            return "ë†’ìŒ"
        elif risk_score >= 3:
            return "ì¤‘ê°„"
        else:
            return "ë‚®ìŒ"
    
    def _get_common_causes(self, class_name):
        """í´ë˜ìŠ¤ë³„ ì¼ë°˜ì ì¸ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì›ì¸"""
        causes = {
            'java.lang.String': [
                "ëŒ€ëŸ‰ì˜ ë¬¸ìì—´ ìƒì„± í›„ ì°¸ì¡° í•´ì œ ì•ˆë¨",
                "ë¬¸ìì—´ ìºì‹±ìœ¼ë¡œ ì¸í•œ ëˆ„ì ",
                "ë¡œê¹…ì´ë‚˜ ë””ë²„ê¹…ìš© ë¬¸ìì—´ ëˆ„ì "
            ],
            'java.util.HashMap': [
                "Mapì— ê°ì²´ ì¶”ê°€ í›„ ì œê±° ì•ˆë¨",
                "í‚¤ë‚˜ ê°’ì˜ ì°¸ì¡°ê°€ ê³„ì† ìœ ì§€ë¨",
                "ìºì‹œë¡œ ì‚¬ìš©ë˜ëŠ” Mapì˜ í¬ê¸° ì œí•œ ì—†ìŒ"
            ],
            'java.util.ArrayList': [
                "ë¦¬ìŠ¤íŠ¸ì— ê°ì²´ ì¶”ê°€ í›„ ì œê±° ì•ˆë¨",
                "ë¦¬ìŠ¤íŠ¸ í¬ê¸°ê°€ ê³„ì† ì¦ê°€",
                "ì„ì‹œ ë°ì´í„°ê°€ ì •ë¦¬ë˜ì§€ ì•ŠìŒ"
            ],
            'java.util.concurrent.ConcurrentHashMap': [
                "ë™ì‹œì„± ì»¬ë ‰ì…˜ì˜ í¬ê¸° ì œí•œ ì—†ìŒ",
                "ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ ì°¸ì¡° ëˆ„ì ",
                "ìºì‹œ ì •ì±… ë¶€ì¬"
            ],
            'java.lang.Object[]': [
                "ë°°ì—´ í¬ê¸°ê°€ ê³„ì† ì¦ê°€",
                "ë°°ì—´ ì°¸ì¡°ê°€ í•´ì œë˜ì§€ ì•ŠìŒ",
                "ëŒ€ìš©ëŸ‰ ë°°ì—´ì˜ ë¶ˆí•„ìš”í•œ ë³´ê´€"
            ]
        }
        
        for key, value in causes.items():
            if key in class_name:
                return value
        
        return ["ì¼ë°˜ì ì¸ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íŒ¨í„´ í™•ì¸ í•„ìš”"]
    
    def _analyze_reference_paths(self):
        """ì°¸ì¡° ê²½ë¡œ ë¶„ì„"""
        # ì‹¤ì œ HPROF íŒŒì‹±ì—ì„œëŠ” ë³µì¡í•˜ë¯€ë¡œ ì‹œë®¬ë ˆì´ì…˜
        for hog in self.memory_hogs:
            class_name = hog['class_name']
            
            # ì¼ë°˜ì ì¸ ì°¸ì¡° ê²½ë¡œ íŒ¨í„´ ìƒì„±
            reference_paths = self._generate_reference_paths(class_name)
            self.reference_paths[class_name] = reference_paths
    
    def _generate_reference_paths(self, class_name):
        """í´ë˜ìŠ¤ë³„ ì°¸ì¡° ê²½ë¡œ íŒ¨í„´ ìƒì„±"""
        common_paths = {
            'java.lang.String': [
                "Application â†’ Service â†’ Cache â†’ String[]",
                "Thread â†’ Logger â†’ StringBuffer â†’ String",
                "Session â†’ UserData â†’ String"
            ],
            'java.util.HashMap': [
                "Application â†’ CacheManager â†’ HashMap",
                "Service â†’ Configuration â†’ HashMap",
                "Thread â†’ LocalCache â†’ HashMap"
            ],
            'java.util.ArrayList': [
                "Controller â†’ Service â†’ ArrayList",
                "Processor â†’ DataList â†’ ArrayList",
                "Manager â†’ TaskQueue â†’ ArrayList"
            ],
            'java.util.concurrent.ConcurrentHashMap': [
                "Application â†’ GlobalCache â†’ ConcurrentHashMap",
                "Service â†’ SessionStore â†’ ConcurrentHashMap",
                "Manager â†’ ResourcePool â†’ ConcurrentHashMap"
            ],
            'java.lang.Object[]': [
                "Service â†’ DataProcessor â†’ Object[]",
                "Controller â†’ ResponseBuilder â†’ Object[]",
                "Manager â†’ ItemList â†’ Object[]"
            ]
        }
        
        for key, paths in common_paths.items():
            if key in class_name:
                return paths
        
        return [
            f"Application â†’ Service â†’ {class_name}",
            f"Manager â†’ DataStore â†’ {class_name}",
            f"Processor â†’ Cache â†’ {class_name}"
        ]
    
    def generate_memory_report(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš© ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.heap_data:
            return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        analysis = self.analyze_old_generation_leaks()
        
        report = f"""
# Java í™ ë¤í”„ ë¶„ì„ ë¦¬í¬íŠ¸

## ğŸ“Š ì „ì²´ ë©”ëª¨ë¦¬ í˜„í™©
- **ì´ ê°ì²´ ìˆ˜**: {analysis['total_objects']:,}ê°œ
- **ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {analysis['total_size_mb']:.2f} MB
- **ë¶„ì„ ì‹œê°„**: {self.heap_data['analysis_time']}

## ğŸ” Old Generation ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì˜ì‹¬ ê°ì²´

"""
        
        if analysis['leak_suspects']:
            for suspect in sorted(analysis['leak_suspects'], key=lambda x: x['size_mb'], reverse=True)[:10]:
                report += f"""
### {suspect['class_name']}
- **ì˜ì‹¬ ì‚¬ìœ **: {suspect['reason']}
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {suspect['size_mb']:.2f} MB
- **ì¸ìŠ¤í„´ìŠ¤ ìˆ˜**: {suspect['instance_count']:,}ê°œ
"""
        else:
            report += "í˜„ì¬ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì˜ì‹¬ ê°ì²´ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        return report


# Streamlit UI
st.set_page_config(
    page_title="Java í™ ë¤í”„ ë¶„ì„ê¸°",
    page_icon="â˜•",
    layout="wide"
)

st.title("â˜• Java í™ ë¤í”„ ë¶„ì„ê¸°")
st.caption("Old Generation ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¶„ì„ ë„êµ¬")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
    
    # ë¶„ì„ ì˜µì…˜
    show_detailed_analysis = st.checkbox("ìƒì„¸ ë¶„ì„ í‘œì‹œ", value=True)
    min_memory_threshold = st.slider("ìµœì†Œ ë©”ëª¨ë¦¬ ì„ê³„ê°’ (MB)", 0.1, 100.0, 1.0, 0.1)
    min_instance_threshold = st.slider("ìµœì†Œ ì¸ìŠ¤í„´ìŠ¤ ì„ê³„ê°’", 100, 50000, 1000, 100)

# ë©”ì¸ ì»¨í…ì¸ 
tabs = st.tabs(["ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", "ğŸ“Š ë¶„ì„ ê²°ê³¼", "ğŸ” ìƒì„¸ ë¶„ì„", "ğŸš¨ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¶„ì„", "ğŸ“ˆ ì‹œê°í™”"])

with tabs[0]:
    st.subheader("í™ ë¤í”„ íŒŒì¼ ì—…ë¡œë“œ")
    
    uploaded_file = st.file_uploader(
        "HPROF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=['hprof', 'gz'],
        help="Java ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ìƒì„±ëœ .hprof íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (ìµœëŒ€ 300MB)"
    )
    
    if uploaded_file is not None:
        # íŒŒì¼ ì •ë³´ í‘œì‹œ
        st.info(f"íŒŒì¼ëª…: {uploaded_file.name}, í¬ê¸°: {uploaded_file.size / (1024*1024):.2f} MB")
        
        # íŒŒì¼ í—¤ë” ì •ë³´ ë¯¸ë¦¬ë³´ê¸°
        file_content = uploaded_file.read()
        uploaded_file.seek(0)  # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
        
        # íŒŒì¼ í—¤ë” í™•ì¸
        if len(file_content) > 20:
            header = file_content[:20].decode('ascii', errors='ignore')
            st.info(f"íŒŒì¼ í—¤ë”: {header}")
            
            # íŒŒì¼ì´ gzipì¸ì§€ í™•ì¸
            if file_content.startswith(b'\x1f\x8b'):
                st.info("íŒŒì¼ì´ gzipìœ¼ë¡œ ì••ì¶•ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            else:
                st.info("íŒŒì¼ì´ ì••ì¶•ë˜ì§€ ì•Šì€ ë°”ì´ë„ˆë¦¬ í˜•ì‹ì…ë‹ˆë‹¤.")
        
        # ë¶„ì„ ì‹¤í–‰
        if st.button("ğŸ” ë¶„ì„ ì‹œì‘", type="primary"):
            with st.spinner("í™ ë¤í”„ íŒŒì¼ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                analyzer = HeapDumpAnalyzer()
                
                # íŒŒì¼ ë‚´ìš© ì½ê¸°
                file_content = uploaded_file.read()
                
                # íŒŒì‹± ì‹¤í–‰
                if analyzer.parse_hprof_file(file_content):
                    st.session_state['analyzer'] = analyzer
                    st.session_state['analysis_complete'] = True
                    st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                else:
                    st.error("íŒŒì¼ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

with tabs[1]:
    st.subheader("ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    
    if 'analyzer' in st.session_state and st.session_state.get('analysis_complete'):
        analyzer = st.session_state['analyzer']
        analysis = analyzer.analyze_old_generation_leaks()
        
        # ì „ì²´ í†µê³„
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ì´ ê°ì²´ ìˆ˜", f"{analysis['total_objects']:,}")
        with col2:
            st.metric("ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰", f"{analysis['total_size_mb']:.2f} MB")
        with col3:
            st.metric("í´ë˜ìŠ¤ ìˆ˜", f"{len(analysis['class_analysis']):,}")
        with col4:
            st.metric("ëˆ„ìˆ˜ ì˜ì‹¬ ê°ì²´", f"{len(analysis['leak_suspects'])}")
        
        # ëˆ„ìˆ˜ ì˜ì‹¬ ê°ì²´ í…Œì´ë¸”
        if analysis['leak_suspects']:
            st.subheader("ğŸš¨ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì˜ì‹¬ ê°ì²´")
            
            leak_df = pd.DataFrame(analysis['leak_suspects'])
            leak_df = leak_df.sort_values('size_mb', ascending=False)
            
            st.dataframe(
                leak_df,
                use_container_width=True,
                column_config={
                    "class_name": "í´ë˜ìŠ¤ëª…",
                    "reason": "ì˜ì‹¬ ì‚¬ìœ ",
                    "size_mb": st.column_config.NumberColumn("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)", format="%.2f"),
                    "instance_count": st.column_config.NumberColumn("ì¸ìŠ¤í„´ìŠ¤ ìˆ˜", format="%d")
                }
            )
        else:
            st.info("ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì˜ì‹¬ ê°ì²´ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    else:
        st.info("ë¨¼ì € í™ ë¤í”„ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

with tabs[2]:
    st.subheader("ìƒì„¸ ë¶„ì„")
    
    if 'analyzer' in st.session_state and st.session_state.get('analysis_complete'):
        analyzer = st.session_state['analyzer']
        analysis = analyzer.analyze_old_generation_leaks()
        
        # í´ë˜ìŠ¤ë³„ ìƒì„¸ ë¶„ì„
        if analysis['class_analysis']:
            st.subheader("ğŸ“‹ í´ë˜ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰")
            
            class_data = []
            for class_name, class_info in analysis['class_analysis'].items():
                if (class_info['total_size'] / (1024*1024)) >= min_memory_threshold or \
                   class_info['instance_count'] >= min_instance_threshold:
                    class_data.append({
                        'í´ë˜ìŠ¤ëª…': class_name,
                        'ì¸ìŠ¤í„´ìŠ¤ ìˆ˜': class_info['instance_count'],
                        'ì´ ë©”ëª¨ë¦¬ (MB)': class_info['total_size'] / (1024*1024),
                        'í‰ê·  í¬ê¸° (bytes)': class_info['avg_size'],
                        'ë©”ëª¨ë¦¬ ë¹„ìœ¨ (%)': class_info['size_percentage']
                    })
            
            if class_data:
                class_df = pd.DataFrame(class_data)
                class_df = class_df.sort_values('ì´ ë©”ëª¨ë¦¬ (MB)', ascending=False)
                
                st.dataframe(
                    class_df,
                    use_container_width=True,
                    column_config={
                        "ì´ ë©”ëª¨ë¦¬ (MB)": st.column_config.NumberColumn(format="%.2f"),
                        "í‰ê·  í¬ê¸° (bytes)": st.column_config.NumberColumn(format="%.0f"),
                        "ë©”ëª¨ë¦¬ ë¹„ìœ¨ (%)": st.column_config.NumberColumn(format="%.2f")
                    }
                )
            else:
                st.info(f"ì„¤ì •ëœ ì„ê³„ê°’({min_memory_threshold}MB, {min_instance_threshold}ê°œ)ì„ ë§Œì¡±í•˜ëŠ” í´ë˜ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìƒì„¸ ë¦¬í¬íŠ¸
        if show_detailed_analysis:
            st.subheader("ğŸ“„ ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸")
            report = analyzer.generate_memory_report()
            st.markdown(report)
    
    else:
        st.info("ë¨¼ì € í™ ë¤í”„ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

with tabs[3]:
    st.subheader("ğŸš¨ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¶„ì„")
    
    if 'analyzer' in st.session_state and st.session_state.get('analysis_complete'):
        analyzer = st.session_state['analyzer']
        analysis = analyzer.analyze_old_generation_leaks()
        
        if analysis['memory_hogs']:
            st.subheader("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìƒìœ„ ê°ì²´ë“¤")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì€ ê°ì²´ë“¤ í…Œì´ë¸”
            memory_hogs_data = []
            for hog in analysis['memory_hogs']:
                memory_hogs_data.append({
                    "ìˆœìœ„": hog['rank'],
                    "í´ë˜ìŠ¤ëª…": hog['class_name'],
                    "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)": f"{hog['total_size_mb']:.2f}",
                    "ì¸ìŠ¤í„´ìŠ¤ ìˆ˜": f"{hog['instance_count']:,}",
                    "í‰ê·  í¬ê¸° (bytes)": f"{hog['avg_size_bytes']:.0f}",
                    "ë©”ëª¨ë¦¬ ë¹„ìœ¨ (%)": f"{hog['size_percentage']:.2f}",
                    "ëˆ„ìˆ˜ ìœ„í—˜ë„": hog['leak_risk']
                })
            
            memory_hogs_df = pd.DataFrame(memory_hogs_data)
            st.dataframe(memory_hogs_df, use_container_width=True)
            
            # ìƒì„¸ ë¶„ì„
            st.subheader("ğŸ” ìƒì„¸ ë¶„ì„")
            
            for hog in analysis['memory_hogs'][:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                with st.expander(f"#{hog['rank']} {hog['class_name']} - {hog['total_size_mb']:.2f}MB"):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**ğŸ“Š í†µê³„ ì •ë³´**")
                        st.write(f"- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {hog['total_size_mb']:.2f} MB")
                        st.write(f"- ì¸ìŠ¤í„´ìŠ¤ ìˆ˜: {hog['instance_count']:,}ê°œ")
                        st.write(f"- í‰ê·  í¬ê¸°: {hog['avg_size_bytes']:.0f} bytes")
                        st.write(f"- ë©”ëª¨ë¦¬ ë¹„ìœ¨: {hog['size_percentage']:.2f}%")
                        st.write(f"- ëˆ„ìˆ˜ ìœ„í—˜ë„: **{hog['leak_risk']}**")
                    
                    with col2:
                        st.write("**âš ï¸ ì¼ë°˜ì ì¸ ì›ì¸**")
                        for cause in hog['common_causes']:
                            st.write(f"â€¢ {cause}")
                    
                    # ì°¸ì¡° ê²½ë¡œ í‘œì‹œ
                    if hog['class_name'] in analysis['reference_paths']:
                        st.write("**ğŸ”— ì°¸ì¡° ê²½ë¡œ**")
                        for path in analysis['reference_paths'][hog['class_name']]:
                            st.code(path)
            
            # ëˆ„ìˆ˜ ìœ„í—˜ë„ë³„ ë¶„ë¥˜
            st.subheader("âš ï¸ ëˆ„ìˆ˜ ìœ„í—˜ë„ë³„ ë¶„ë¥˜")
            
            high_risk = [hog for hog in analysis['memory_hogs'] if hog['leak_risk'] == 'ë†’ìŒ']
            medium_risk = [hog for hog in analysis['memory_hogs'] if hog['leak_risk'] == 'ì¤‘ê°„']
            low_risk = [hog for hog in analysis['memory_hogs'] if hog['leak_risk'] == 'ë‚®ìŒ']
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.error(f"**ë†’ìŒ ìœ„í—˜: {len(high_risk)}ê°œ**")
                for hog in high_risk:
                    st.write(f"â€¢ {hog['class_name']} ({hog['total_size_mb']:.2f}MB)")
            
            with col2:
                st.warning(f"**ì¤‘ê°„ ìœ„í—˜: {len(medium_risk)}ê°œ**")
                for hog in medium_risk:
                    st.write(f"â€¢ {hog['class_name']} ({hog['total_size_mb']:.2f}MB)")
            
            with col3:
                st.success(f"**ë‚®ìŒ ìœ„í—˜: {len(low_risk)}ê°œ**")
                for hog in low_risk:
                    st.write(f"â€¢ {hog['class_name']} ({hog['total_size_mb']:.2f}MB)")
            
            # ê¶Œì¥ì‚¬í•­
            st.subheader("ğŸ’¡ ê¶Œì¥ì‚¬í•­")
            
            if high_risk:
                st.error("**ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”:**")
                st.write("1. ìƒìœ„ ìœ„í—˜ ê°ì²´ë“¤ì˜ ì°¸ì¡° ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”")
                st.write("2. ë¶ˆí•„ìš”í•œ ê°ì²´ ì°¸ì¡°ë¥¼ í•´ì œí•˜ëŠ” ë¡œì§ì„ ì¶”ê°€í•˜ì„¸ìš”")
                st.write("3. ìºì‹œ í¬ê¸° ì œí•œì´ë‚˜ TTL(Time To Live) ì •ì±…ì„ ë„ì…í•˜ì„¸ìš”")
            
            if medium_risk:
                st.warning("**ëª¨ë‹ˆí„°ë§ í•„ìš”:**")
                st.write("1. í•´ë‹¹ ê°ì²´ë“¤ì˜ ì‚¬ìš© íŒ¨í„´ì„ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”")
                st.write("2. ì •ê¸°ì ì¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ë¡œì§ì„ ê²€í† í•˜ì„¸ìš”")
            
            if not high_risk and not medium_risk:
                st.success("**ì–‘í˜¸í•œ ìƒíƒœ:**")
                st.write("í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš© íŒ¨í„´ì´ ì–‘í˜¸í•©ë‹ˆë‹¤. ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ìœ ì§€í•˜ì„¸ìš”.")
        
        else:
            st.info("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì€ ê°ì²´ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    else:
        st.info("ë¨¼ì € í™ ë¤í”„ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

with tabs[4]:
    st.subheader("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹œê°í™”")
    
    if 'analyzer' in st.session_state and st.session_state.get('analysis_complete'):
        analyzer = st.session_state['analyzer']
        analysis = analyzer.analyze_old_generation_leaks()
        
        if analysis['class_analysis']:
            # ìƒìœ„ 20ê°œ í´ë˜ìŠ¤ë§Œ ì‹œê°í™”
            top_classes = sorted(
                analysis['class_analysis'].items(),
                key=lambda x: x[1]['total_size'],
                reverse=True
            )[:20]
            
            if top_classes:
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì°¨íŠ¸
                class_names = [name for name, _ in top_classes]
                memory_sizes = [info['total_size'] / (1024*1024) for _, info in top_classes]
                
                fig_memory = px.bar(
                    x=memory_sizes,
                    y=class_names,
                    orientation='h',
                    title="í´ë˜ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ìƒìœ„ 20ê°œ)",
                    labels={'x': 'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)', 'y': 'í´ë˜ìŠ¤ëª…'}
                )
                fig_memory.update_layout(height=600)
                st.plotly_chart(fig_memory, use_container_width=True)
                
                # ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ì°¨íŠ¸
                instance_counts = [info['instance_count'] for _, info in top_classes]
                
                fig_instances = px.bar(
                    x=instance_counts,
                    y=class_names,
                    orientation='h',
                    title="í´ë˜ìŠ¤ë³„ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ (ìƒìœ„ 20ê°œ)",
                    labels={'x': 'ì¸ìŠ¤í„´ìŠ¤ ìˆ˜', 'y': 'í´ë˜ìŠ¤ëª…'}
                )
                fig_instances.update_layout(height=600)
                st.plotly_chart(fig_instances, use_container_width=True)
                
                # íŒŒì´ ì°¨íŠ¸ (ë©”ëª¨ë¦¬ ë¹„ìœ¨)
                if len(top_classes) > 1:
                    fig_pie = px.pie(
                        values=memory_sizes,
                        names=class_names,
                        title="ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„í¬ (ìƒìœ„ 20ê°œ í´ë˜ìŠ¤)"
                    )
                    st.plotly_chart(fig_pie, use_container_width=True)
        else:
            st.info("ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    else:
        st.info("ë¨¼ì € í™ ë¤í”„ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

# í‘¸í„°
st.markdown("---")
st.caption("ğŸ’¡ **ì‚¬ìš© íŒ**: Old Generation ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ëŠ” ë³´í†µ ëŒ€ëŸ‰ì˜ ê°ì²´ê°€ ìƒì„±ë˜ì–´ GCë˜ì§€ ì•ŠëŠ” ê²½ìš° ë°œìƒí•©ë‹ˆë‹¤. ì˜ì‹¬ ê°ì²´ë¥¼ í™•ì¸í•˜ì—¬ ë©”ëª¨ë¦¬ í•´ì œ ë¡œì§ì„ ì ê²€í•´ë³´ì„¸ìš”.")
